# -*- coding: utf-8 -*-
"""feature_extraction_NRC_for_(SEMEVAL_DATASET)_(8_12_2021)_(1).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1AGWtwZndPsGJXSG9hFdbW8RhYGkJJ2eM

# Import Important Python Libararies
"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
import pandas as pd
import numpy as np
import nltk
import future
from sklearn.naive_bayes import MultinomialNB
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import roc_auc_score, accuracy_score
from sklearn.linear_model import LogisticRegression
from sklearn import svm
import re
from sklearn import metrics

"""# Read Dataset"""

df = pd.read_csv(r"Dataset sem el oc - tweet_label.csv")
df.head(5)

"""# Read Lexicon Data"""

lexicon = pd.read_csv(r"NRC Lexicon.csv")
lexicon.head(5)

lexicon.keys()

"""# Data Cleaning"""

df['clean_tweet'] = df.tweet.str.replace('[^\w\s#@/:%.,_-]', '', flags=re.UNICODE) #removes emojis
df['clean_tweet'] = df.clean_tweet.str.replace('@[_A-Za-z0-9]+', '') #removes handles
df['clean_tweet'] = df.clean_tweet.str.replace('[A-Za-z0-9]+', '') #removes english
df['clean_tweet'] = df.clean_tweet.str.replace('#',' ') #removes hashtag symbol only
df['clean_tweet'] = df.clean_tweet.str.replace(r'http\S+', '', regex=True).replace(r'www\S+', '', regex=True) #removes links
df['clean_tweet'] = df.clean_tweet.str.replace('\d+', '') #removes numbers
df['clean_tweet'] = df.clean_tweet.str.replace('\n', ' ') #removes new line
df['clean_tweet'] = df.clean_tweet.str.replace('_', '') #removes underscore
df['clean_tweet'] = df.clean_tweet.str.replace('[^\w\s]','') #removes punctuation
df.tail(5)

"""# Stopwords Removal"""

stop_words_removal = True

if stop_words_removal:
    import nltk
    nltk.download("stopwords") 
    arb_stopwords = set(nltk.corpus.stopwords.words("arabic"))
    stopwords = r'\b(?:{})\b'.format('|'.join(arb_stopwords))
    df['clean_tweet_no_stop'] = df['clean_tweet'].str.replace(stopwords, '')
else:
    df['clean_tweet_no_stop'] = df['clean_tweet']

import nltk
nltk.download('punkt')
nltk.download('wordnet')

stemmer = nltk.ISRIStemmer()
lemmatizer = nltk.WordNetLemmatizer()
def tokenization(txt):
		"""
			tokenization a certain arabic text
			:param txt: string : arabic text
			:return: tokens : array : array contain Tokens
		"""
		tokens = nltk.word_tokenize(txt)
		return tokens
def stemming(txt):
		"""
			Apply Arabic Stemming without a root dictionary, using nltk's ISRIStemmer.
			:param txt: string : arabic text
			:return: stems : array : array contains a stem for each word in the text
		"""
		stems = ' '.join([stemmer.stem(w) for w in tokenization(txt)])
		return stems

def lemmatization(txt):
		"""
			Lemmatize using WordNet's morphy function.
			Returns the input word unchanged if it cannot be found in WordNet.
			:param txt: string : arabic text
			:return: lemmas : array : array contains a Lemma for each word in the text.
		"""
		lemmas = ' '.join([lemmatizer.lemmatize(w) for w in tokenization(txt)])
		return lemmas

df["clean_tweet_lemitized"] = df.apply(lambda row : lemmatization(row['clean_tweet_no_stop']), axis = 1)
df[["clean_tweet_no_stop", "clean_tweet_lemitized"]]

df["clean_tweet_stemmed"] = df.apply(lambda row : stemming(row['clean_tweet_lemitized']), axis = 1)
df[["clean_tweet_no_stop", "clean_tweet_stemmed"]]

lexicon["الفئة"].value_counts()

df["label"].value_counts()

# Assign Column - All values initailly 0
df["lexicon_score_anger"] = 0
df["lexicon_score_fear"] = 0
df["lexicon_score_joy"] = 0
df["lexicon_score_sadness"] = 0
# df["lexicon_score_trust"] = 0
# df["lexicon_score_disgust"] = 0
# df["lexicon_score_surprise"] = 0
# df["lexicon_score_anticipation"] = 0

df.head(5)

"""# Functions to calculate Feathers"""

def calcExtraFeaturehappy(query):
    lexicon_score_joy = 0
    
    # For each word in Tweet
    for i in query.split(" "):
        try:
            # Search for the happy values - - If available get its PFA values and added to score
            sc1 = lexicon[lexicon["الفئة"] == "فرح"].loc[lexicon["كلمة"] == i][" PFA"].values[0]
            lexicon_score_joy += sc1
        except:
            # May be lexicon not available, just skip
            pass
        
    return lexicon_score_joy

def calcExtraFeatureAnger(query):
    lexicon_score_anger = 0
    for i in query.split(" "):
        try:
            # Search for the Anger lexicons - - If available get its PFA values and added to score
            sc2 = lexicon[lexicon["الفئة"] == "غضب"].loc[lexicon["كلمة"] == i][" PFA"].values[0]
            lexicon_score_anger += sc2
        except:
            pass
        
    return lexicon_score_anger

def calcExtraFeatureFear(query):
    lexicon_score_fear = 0
    for i in query.split(" "):
        try:
            # Search for the Fear lexicons - - If available get its PFA values and added to score
            sc3 = lexicon[lexicon["الفئة"] == "خوف"].loc[lexicon["كلمة"] == i][" PFA"].values[0]
            lexicon_score_fear += sc3
        except:
            pass
    return lexicon_score_fear

def calcExtraFeatureSad(query):
    lexicon_score_sadness = 0
    for i in query.split(" "):
        try:
            sc4 = lexicon[lexicon["الفئة"] == "حزن"].loc[lexicon["كلمة"] == i][" PFA"].values[0]
            lexicon_score_sadness += sc4
        except:
            pass
    return lexicon_score_sadness

df["lexicon_score_anger"] = df.apply(lambda row : calcExtraFeatureAnger(row['clean_tweet_stemmed']), axis = 1)

df["lexicon_score_fear"] = df.apply(lambda row : calcExtraFeatureFear(row['clean_tweet_stemmed']), axis = 1)

df["lexicon_score_joy"] = df.apply(lambda row : calcExtraFeaturehappy(row['clean_tweet_stemmed']), axis = 1)

df["lexicon_score_sadness"] = df.apply(lambda row : calcExtraFeatureSad(row['clean_tweet_stemmed']), axis = 1)

# df["lexicon_score_disgust"] = df.apply(lambda row : calcExtraFeaturedisgust(row['clean_tweet_stemmed']), axis = 1)

# df["lexicon_score_trust"] = df.apply(lambda row : calcExtraFeaturetrust(row['clean_tweet_stemmed']), axis = 1)

# df["lexicon_score_surprise"] = df.apply(lambda row : calcExtraFeaturesurprise(row['clean_tweet_stemmed']), axis = 1)

# df["lexicon_score_anticipation"] = df.apply(lambda row : calcExtraFeatureanticipation(row['clean_tweet_stemmed']), axis = 1)

df

df.to_csv('feature extraction with NRC(SEM-EL-OC DATASET)', header=True, index=False, encoding='utf-8-sig')

