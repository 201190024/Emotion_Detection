# -*- coding: utf-8 -*-
"""DEEP_Learning_(Embedding_Layer)_(SEM_ELOC_22_1_2022).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1h1RAH_Zu7bevdcep_3N-Dn5q-gC2Jypr
"""

import re
import matplotlib.pyplot as plt
import string
from nltk.corpus import stopwords
import nltk
from nltk.corpus import wordnet
from nltk.stem import WordNetLemmatizer
from nltk.tokenize.treebank import TreebankWordDetokenizer
from collections import Counter
from wordcloud import WordCloud
from nltk.corpus import stopwords
import nltk
from gensim.utils import simple_preprocess
from nltk.corpus import stopwords
import gensim
from sklearn.model_selection import train_test_split
import spacy
import pickle
import warnings
warnings.filterwarnings('ignore')
import seaborn as sns
from sklearn.metrics import confusion_matrix
import matplotlib.pyplot as plt 
import tensorflow as tf
import keras
import numpy as np
import pandas as pd
print('Done')

"""# Data importing"""

train = pd.read_csv(r"SEM-EL-OC DATASET.csv")

"""# Data exploration"""

train.head(5)

len(train)

train['label'].unique()

"""# Data cleaning"""

#Let's keep only the columns that we're going to use
df = train[['clean_tweet_stemmed','label']]
df.head()

clean_tweet_stemmed = df['clean_tweet_stemmed'].values.astype('U')

labels = np.array(df['label'])
y = []
for i in range(len(df["label"])):
    if labels[i] == 'anger':
        y.append(0)
    if labels[i] == 'sadness':
        y.append(1)
    if labels[i] == 'fear':
        y.append(2)
    if labels[i] == 'joy':
        y.append(3)



y = np.array(y)
labels = tf.keras.utils.to_categorical(y, 4, dtype="float32")
del y

"""# Data sequencing and splitting

"""

from keras.models import Sequential
from keras import layers
# from keras.optimizers import RMSprop,Adam
from tensorflow.keras.optimizers import RMSprop
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from keras import regularizers
from keras import backend as K
from keras.callbacks import ModelCheckpoint

# Number of words in tokens vector
max_words = 5000
max_len = 200


tokenizer = Tokenizer(num_words=max_words)
tokenizer.fit_on_texts(clean_tweet_stemmed)
sequences = tokenizer.texts_to_sequences(clean_tweet_stemmed)
tweets = pad_sequences(sequences, maxlen=max_len)
print(tweets)

# Randomly shuffling data
from sklearn.utils import shuffle
df = shuffle(df)
df.head(5)

tweets.shape

print(labels)

#Splitting the data
X_train, X_test, y_train, y_test = train_test_split(tweets,labels, test_size=0.3, random_state=42,stratify=labels)
print (len(X_train),len(X_test),len(y_train),len(y_test))

"""# Model building

## Single LSTM layer model
"""

# create model object
model1 = Sequential()

# adding layer
model1.add(layers.Embedding(max_words, 20))

# Adding LSTM layer
model1.add(layers.LSTM(128,dropout=0.5))

# Adding dense layer with softmax activation
model1.add(layers.Dense(4,activation='softmax'))

optimizer = tf.keras.optimizers.RMSprop(
     learning_rate=0.001,
     rho=0.9,
     momentum=0.0,
     epsilon=1e-07,
     centered=False,
     name="RMSprop")

model1.compile(optimizer=optimizer,loss='categorical_crossentropy', metrics=['accuracy'])

#Implementing model checkpoins to save the best metric and do not lose it on training.
checkpoint1 = ModelCheckpoint("best_model1.hdf5", monitor='val_accuracy', verbose=1,save_best_only=True, mode='auto', period=1,save_weights_only=False)

history = model1.fit(X_train, y_train, epochs=20, batch_size= 8, validation_data=(X_test, y_test),callbacks=[checkpoint1])

best_model = keras.models.load_model("best_model1.hdf5")
test_loss, test_acc = best_model.evaluate(X_test, y_test, verbose=2)
print('Model accuracy: ',test_acc)

predictions = best_model.predict(X_test)

from sklearn.metrics import accuracy_score
from sklearn.metrics import classification_report
print(classification_report(np.argmax(y_test, axis=1), np.argmax(predictions, axis=1)))

"""## Bidirectional LTSM model"""

# Model
model2 = Sequential()

# adding layer
model2.add(layers.Embedding(max_words, 40, input_length=max_len))

# Adding bidirectional LSTM layer
model2.add(layers.Bidirectional(layers.LSTM(128,dropout=0.5)))

# Adding dense layer with softmax activation
model2.add(layers.Dense(4,activation='softmax'))

optimizer = tf.keras.optimizers.RMSprop(
     learning_rate=0.001,
     rho=0.9,
     momentum=0.0,
     epsilon=1e-07,
     centered=False,
     name="RMSprop")


model2.compile(optimizer=optimizer,loss='categorical_crossentropy', metrics=['accuracy'])

#Implementing model checkpoins to save the best metric and do not lose it on training.
checkpoint2 = ModelCheckpoint("best_model2.hdf5", monitor='val_accuracy', verbose=1,save_best_only=True, mode='auto', period=1,save_weights_only=False)

history = model2.fit(X_train, y_train, epochs=20, batch_size= 16, validation_data=(X_test, y_test),callbacks=[checkpoint2])

best_model = keras.models.load_model("best_model2.hdf5")
test_loss, test_acc = best_model.evaluate(X_test, y_test, verbose=2)
print('Model accuracy: ',test_acc)

predictions = best_model.predict(X_test)

from sklearn.metrics import accuracy_score
from sklearn.metrics import classification_report
print(classification_report(np.argmax(y_test, axis=1), np.argmax(predictions, axis=1)))

from keras import regularizers
model4 = Sequential()

model4.add(layers.Embedding(max_words, 80, input_length=max_len))

model4.add(layers.Conv1D(128, 6, activation='relu'))
model4.add(layers.MaxPooling1D(5))
model4.add(layers.Conv1D(64, 6, activation='relu'))

model4.add(layers.GlobalMaxPooling1D())

model4.add(layers.Dense(4,activation='softmax'))

optimizer = tf.keras.optimizers.RMSprop(
     learning_rate=0.002,
     rho=0.9,
     momentum=0.0,
     epsilon=1e-07,
     centered=False,
     name="RMSprop")

model3.compile(optimizer=optimizer,loss='categorical_crossentropy',metrics=['accuracy'])
checkpoint3 = ModelCheckpoint("best_model4.hdf5", monitor='val_accuracy', verbose=1,save_best_only=True, mode='auto', period=1,save_weights_only=False)
history = model3.fit(X_train, y_train, epochs=20, batch_size= 8,validation_data=(X_test, y_test), callbacks=[checkpoint3])

best_model4 = keras.models.load_model("best_model4.hdf5")
test_loss, test_acc = best_model4.evaluate(X_test, y_test, verbose=2)
print('Model accuracy: ',test_acc)

predictions = best_model4.predict(X_test)

from sklearn.metrics import accuracy_score
from sklearn.metrics import classification_report
print(classification_report(np.argmax(y_test, axis=1), np.argmax(predictions, axis=1)))

